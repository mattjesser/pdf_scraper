{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Enter Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the files\n",
    "# THESE ARE THE ONLY 3 THINGS YOU NEED TO CHANGE \n",
    "\n",
    "base_path = \"C:/Users/matt/Jesser Analytics Consulting/Natsco Transit Solutions/python/\"\n",
    "filename = 'NOVA_L998'\n",
    "pg = {'pgstart': 39, 'pgend':1431}\n",
    "    #NOVA_L998 ### 39 - 1431\n",
    "    #NF_SR2089 ### 91 - 1451\n",
    "    #NF_SR1600 ### 79 - 1409\n",
    "    \n",
    "prm = {'cm':200.0, 'lm':0.1, 'wm':0.1, 'bf':1.0, 'lo':0.9} #defaults: {cm=2.0, lm=0.5, wm=0.1, bf=0.5, lo=0.5}\n",
    "out = {'header':0,'subheader':1,'bodystart':3, 'spaces_desc':3}\n",
    "\n",
    "####################################################\n",
    "\n",
    "# setup additional variables\n",
    "f = f\"{filename}.pdf\"\n",
    "path = f\"{base_path}{filename}\"\n",
    "\n",
    "# setup your pages\n",
    "p = pgprep(pgstart=pg['pgstart'], pgend=pg['pgend'])\n",
    "print(p['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Run the Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize my three dataframes for logs, summary logs, and final dataset\n",
    "l_col=['file','page','charlen','char', 'pos_BLx', 'pos_BLy', 'pos_TRx', 'pos_TRy']\n",
    "s_col=['filename', 'page', 'loglen', 'top', 'bottom']\n",
    "d_col=['file', 'page', 'header', 'subheader', 'localnum',  'revnum', 'pagepos', 'col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col_notes']\n",
    "\n",
    "dfl = pd.DataFrame(columns=l_col)\n",
    "dfs = pd.DataFrame(columns=s_col)\n",
    "df = pd.DataFrame(columns=d_col)\n",
    "\n",
    "\n",
    "# run the whole program for the defined set of pages and create 3 dataframes\n",
    "for p in p['p']:\n",
    "    # Analyze the requested PDF file\n",
    "    pa = pdf_analyze(filename=f, page=p-1, cm=prm['cm'], lm=prm['lm'], wm=prm['wm'], bf=prm['bf'], lo=prm['lo'])\n",
    "                                        #defaults: cm=2.0,   lm=0.5, wm=0.1, bf=0.5, lo=0.5\n",
    "    print(pa['message'])\n",
    "    \n",
    "    # Extract data for the specified page\n",
    "    d = extract_data(filename=f, page=p,layout=pa['layout'])\n",
    "    print(d['message'])\n",
    "\n",
    "    # Create the Summary & Detailed logs for the specified page\n",
    "    log = create_logs(data=d['data'],filename=f, page=p, path=path)\n",
    "    \n",
    "    dfl = dfl.append(log['log'], sort=True) #append logs to total log dataframe\n",
    "    dfs = dfs.append(log['summary'], sort=True) #append summary to total summary dataframe\n",
    "\n",
    "    print(log['message'])\n",
    "\n",
    "    # Format & append the Data for the specified page\n",
    "    data =  create_data(data=d['data'],filename=f,path=path,page=p, header=out['header'], subheader=out['subheader'], body=out['bodystart'], spaces_desc=out['spaces_desc'])    \n",
    "    df = df.append(data['data'], sort=True) #append data to total data dataframe\n",
    "    print(data['message'])\n",
    "        \n",
    "    print()\n",
    "\n",
    "# Reindex to set the columns the right way\n",
    "dfs=dfs.reindex(columns=s_col)\n",
    "dfl=dfl.reindex(columns=l_col)\n",
    "df=df.reindex(columns=d_col)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) View Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dfs #view summary\n",
    "#dfl\n",
    "#dfl['char'][7] #view detailed logs\n",
    "#df #view data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Write Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#osp = osprep(base_path,filename)\n",
    "#print(osp['message'])\n",
    "\n",
    "# create a new folder if it doesn't exist already\n",
    "if os.path.isdir(path):\n",
    "    print('Using existing folder')\n",
    "else:\n",
    "    os.mkdir(path)\n",
    "    print(f'Created a new folder: \"/{filename}\"')\n",
    "\n",
    "data_out_name = os.path.join(path,filename) # create the output path & filename\n",
    "\n",
    "dfs.to_csv(f'{data_out_name} - summary.csv', index=False,  mode='a', header=True)\n",
    "print('write summary')\n",
    "\n",
    "dfl.to_csv(f'{data_out_name} - log.csv', index=False, mode='a', header=True)\n",
    "print('write logs')\n",
    "\n",
    "df.to_csv(f'{data_out_name} - data.csv', index=False, mode='a', header=True)\n",
    "print('write data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Dependencies & Functions we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Import our libraries\n",
    "##################################################\n",
    "\n",
    "# we primarily use 'pdfminer' to parse the pdf files \n",
    "# we use 'pandas' to convert the results into a useful table structure and export\n",
    "\n",
    "import os\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "##################################################\n",
    "# Create OS Functions\n",
    "##################################################\n",
    "\n",
    "def pgprep(pgstart, pgend):\n",
    "    #create your list of pages\n",
    "    try:\n",
    "        if pgstart == pgend:\n",
    "            p = [pgstart]\n",
    "            pass\n",
    "        else:\n",
    "            p = list(range(pgstart, pgend+1))\n",
    "    except:\n",
    "        p = [pgstart]\n",
    "    \n",
    "    return {'message':f'Using {len(p)} pages from {pgstart} to {pgend}', 'p':p} \n",
    "\n",
    "##################################################\n",
    "# Create PDF Parse Functions\n",
    "##################################################\n",
    "    \n",
    "def pdf_analyze (filename, page, lm=0.5, cm=2.0, wm=0.1, bf=0.5, lo=0.5):\n",
    "    # This is the main function for taking a specific page in a PDF file and \n",
    "    # parsing it into a LTpage object that can be further parsed \n",
    "    # the parameters drive the logic for 'where' the parsing happens.\n",
    "    # more documentation on these params can be found in '0) documentation' above\n",
    "    \n",
    "    # Open and read the pdf file in binary mode\n",
    "    fp = open(filename, \"rb\")\n",
    "\n",
    "    # Create parser object to parse the pdf content\n",
    "    parser = PDFParser(fp)\n",
    "\n",
    "    # Store the parsed content in PDFDocument object\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    # Check if document is extractable, if not abort\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "\n",
    "    # Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "\n",
    "    # set parameters for analysis... these come from params in the function\n",
    "    laparams = LAParams(\n",
    "                        line_margin=lm, \n",
    "                        char_margin=cm, \n",
    "                        word_margin=wm,\n",
    "                        boxes_flow=bf,\n",
    "                        line_overlap=lo\n",
    "                        )\n",
    "\n",
    "    \n",
    "    # Create a PDFDevice object which translates interpreted information into desired format\n",
    "    # Device needs to be connected to resource manager to store shared resources\n",
    "    # Extract the decive to page aggregator to get LT object elements\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    \n",
    "\n",
    "    # Create interpreter object to process page content from PDFDocument\n",
    "    # Interpreter needs to be connected to resource manager for shared resources and device \n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # put all pages into a list\n",
    "    pgs = PDFPage.create_pages(document)\n",
    "    allpages = []\n",
    "    for p in pgs:\n",
    "        allpages.append(p)\n",
    "    \n",
    "    # As the interpreter processes the page stored in PDFDocument object\n",
    "    interpreter.process_page(allpages[page])\n",
    "\n",
    "    # The device renders the layout from interpreter\n",
    "    layout = device.get_result()\n",
    "\n",
    "    return {'layout':layout,'message':f'Page {page+1}: Analyzed PDF'}\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(filename, page, layout):\n",
    "    # this function takes the 'layout'object created from the parser and pulls out\n",
    "    # the key pieces that we want, ignoring things like lines, images, etc.\n",
    "\n",
    "    data = []\n",
    "    for obj in layout:\n",
    "        #print(obj)\n",
    "        if isinstance(obj, LTTextBox) or isinstance(obj, LTTextLine):\n",
    "\n",
    "            d = {\n",
    "                'char': obj.get_text().split('\\n'),\n",
    "                'file': filename,\n",
    "                'page': page,\n",
    "                'charlen': len(obj.get_text()),\n",
    "                'pos_BLx': obj.bbox[0],\n",
    "                'pos_BLy': obj.bbox[1],\n",
    "                'pos_TRx': obj.bbox[2],\n",
    "                'pos_TRy': obj.bbox[3]\n",
    "                #'fontname': child.fontname,\n",
    "                #'fontsize': child.size\n",
    "                }\n",
    "            data.append(d)\n",
    "    return {'data':data, 'message':f'Page {page}: Extracted Objects'}\n",
    "\n",
    "##################################################\n",
    "# Create Logs\n",
    "##################################################\n",
    "\n",
    "def create_logs (data, filename, path, page):\n",
    "    # this function creates two files: 'file - log' and 'file - log summary'\n",
    "    #\n",
    "    # 'file - log' provides a breakdown of each container that the parser created, \n",
    "    # allowing the developer to troubleshoot data outputs that aren't correct\n",
    "    #\n",
    "    # 'file - log summary' provides a high level overview by page so we can see which pages\n",
    "    # will parse easily and which onces need more investigation\n",
    "    \n",
    "    try:\n",
    "        # try creating logs. if you get an error, then skip the logs\n",
    "        \n",
    "        #create a dataframe from the data passed to the function\n",
    "        log = pd.DataFrame(data)\n",
    "        \n",
    "        #create a summary dataset to easier understand how the page is structured\n",
    "        summary = {\n",
    "                'filename': filename,\n",
    "                'page': page,\n",
    "                'loglen': len(log),\n",
    "                'top': log['char'][0][0],\n",
    "                'bottom': log['char'][len(log)-1][0]\n",
    "            }\n",
    "\n",
    "        logsum = pd.DataFrame(summary, index=[0])\n",
    "\n",
    "        return {'message': f'Page {page}: Create Logs', 'summary':logsum, 'log':log }\n",
    "    \n",
    "    except:\n",
    "        # if there's some error with the page, don't write anything and skip it\n",
    "        \n",
    "        # create an empty log df\n",
    "        log = pd.DataFrame(columns=['char', 'charlen', 'file', 'page', 'pos_BLx', 'pos_BLy', 'pos_TRx', 'pos_TRy'])\n",
    "        \n",
    "        # create a blank row in the summary data\n",
    "        summary = {\n",
    "                'filename': filename,\n",
    "                'page': page,\n",
    "                'loglen': 0,\n",
    "                'top': '!ERROR',\n",
    "                'bottom': '!ERROR'\n",
    "            }\n",
    "        \n",
    "        logsum = pd.DataFrame(summary, index=[0])\n",
    "\n",
    "        return {'message': f'Page {page}: ERROR! No Logs', 'summary':logsum, 'log':log }\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Create Final Data\n",
    "##################################################\n",
    "    \n",
    "def create_data(data, filename, path, page, header=0, subheader=1, body=5, spaces_desc=2):\n",
    "    # Here we go! Let's create the final table and put it in Excel  \n",
    "    \n",
    "    totrows = len(d['data']) # get the number of unique objects in the pdf\n",
    "    rows_list = [] # create an empty list so we can put each object in it's own row\n",
    "\n",
    "    for r in range(body,totrows-2): # magic happens here. Start looping through each object and append metadata\n",
    "        ###\n",
    "        ### First split the 'char' field into its useful parts ###\n",
    "        ###\n",
    "        \n",
    "        char = d['data'][r]['char'][0]  #get the char piece from the dataset & write to the window       \n",
    "        try:\n",
    "            sp1 = char.split(' ',spaces_desc) #split the first two based on space\n",
    "\n",
    "            # Is this a valid row of data?\n",
    "            test_invalid = (re.match('.[#*()&%$@!+-]', sp1[0]))\n",
    "            if test_invalid:\n",
    "                #sp1 = [char] # if this is invalid, just return the data\n",
    "                bo = {}\n",
    "                bo = {'col_notes':char}\n",
    "            else:\n",
    "                # look for the asterisk\n",
    "                test_ast = (re.match('^([A-Z]|[#*()^&%$@!+-])\\s', sp1[2])) #or (re.match('^QR', sp1[2]))\n",
    "                ast, bd ='',''\n",
    "                if test_ast:\n",
    "                    ast, bd = sp1[len(sp1)-1][:2], sp1[len(sp1)-1][2:]\n",
    "                    sp1.pop(2)\n",
    "                    sp1.extend([bd])\n",
    "\n",
    "                # Break apart the Desc & Qty Fields\n",
    "                test_desqt = (re.match('^[0-9]+', sp1[2])) or (re.match('^QR', sp1[2]))\n",
    "\n",
    "                if test_desqt:\n",
    "                    qty, desc = sp1[len(sp1)-1][:2], sp1[len(sp1)-1][2:]  #split the remaining desc/quantity field\n",
    "                    sp1.pop(2) #get rid of the crappy desc/quantity combo field            \n",
    "                    sp1.extend([desc,qty,ast]) #combine the new desc/qty fields back to the main output\n",
    "                else:\n",
    "                    sp1.extend([ast])\n",
    "\n",
    "                # Convert sp1 into a dict that can be appended\n",
    "                bo = {}\n",
    "                i = 1\n",
    "                for c in sp1:\n",
    "                    bo.update( {f'col{i}' : c} )\n",
    "                    i += 1       \n",
    "        except:\n",
    "            bo = {}\n",
    "        ###\n",
    "        ### First split the 'char' field into its useful parts ###\n",
    "        ###\n",
    "        \n",
    "        mydict = {'file':filename, \n",
    "         'page':page,\n",
    "         'header':d['data'][header]['char'][0],\n",
    "         'subheader':d['data'][subheader]['char'][0],\n",
    "         'localnum':d['data'][totrows-2]['char'][0],\n",
    "         'revnum':d['data'][totrows-1]['char'][0],\n",
    "         'pagepos' :r-body+1,\n",
    "         #'char':char,\n",
    "         #'lenchar':len(d['data'][r]['char'][0])\n",
    "        }\n",
    "        \n",
    "        mydict.update(bo) #combine the metadata with the broken out table data\n",
    "        rows_list.append(mydict)\n",
    "\n",
    "    ###\n",
    "    ### Finalize the data and output to excel if needed\n",
    "    ###\n",
    "    \n",
    "    df = pd.DataFrame(rows_list) # put all the data into a dataframe to make things easy\n",
    "    data_out_name = os.path.join(path,f\"{filename} - data.csv\") # create the output path & filename\n",
    "\n",
    "    return {'message': f'Page {page}: Created Data', 'data':df}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 99) Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gist.github.com/vinovator/c78c2cb63d62fdd9fb67\n",
    "\n",
    "# pdfTextMiner.py\n",
    "# Python 2.7.6\n",
    "# For Python 3.x use pdfminer3k module\n",
    "# This link has useful information on components of the program\n",
    "# https://euske.github.io/pdfminer/programming.html\n",
    "# http://denis.papathanasiou.org/posts/2010.08.04.post.html\n",
    "\n",
    "\n",
    "''' Important classes to remember\n",
    "PDFParser - fetches data from pdf file\n",
    "PDFDocument - stores data parsed by PDFParser\n",
    "PDFPageInterpreter - processes page contents from PDFDocument\n",
    "PDFDevice - translates processed information from PDFPageInterpreter to whatever you need\n",
    "PDFResourceManager - Stores shared resources such as fonts or images used by both PDFPageInterpreter and PDFDevice\n",
    "LAParams - A layout analyzer returns a LTPage object for each page in the PDF document\n",
    "PDFPageAggregator - Extract the device to page aggregator to get LT object elements\n",
    "'''\n",
    "\n",
    "'''\n",
    " Parameters for layout analysis\n",
    "    \n",
    "    :param line_overlap: If two characters have more overlap than this they\n",
    "        are considered to be on the same line. The overlap is specified\n",
    "        relative to the minimum height of both characters.\n",
    "    \n",
    "    :param char_margin: If two characters are closer together than this\n",
    "        margin they are considered to be part of the same word. If\n",
    "        characters are on the same line but not part of the same word, an\n",
    "        intermediate space is inserted. The margin is specified relative to\n",
    "        the width of the character.\n",
    "    \n",
    "    :param word_margin: If two words are are closer together than this\n",
    "        margin they are considered to be part of the same line. A space is\n",
    "        added in between for readability. The margin is specified relative\n",
    "        to the width of the word.\n",
    "    \n",
    "    :param line_margin: If two lines are are close together they are\n",
    "        considered to be part of the same paragraph. The margin is\n",
    "        specified relative to the height of a line.\n",
    "    \n",
    "    :param boxes_flow: Specifies how much a horizontal and vertical position\n",
    "        of a text matters when determining the order of text boxes. The value\n",
    "        should be within the range of -1.0 (only horizontal position\n",
    "        matters) to +1.0 (only vertical position matters).\n",
    "    \n",
    "    :param detect_vertical: If vertical text should be considered during\n",
    "        layout analysis\n",
    "    \n",
    "    :param all_texts: If layout analysis should be performed on text in\n",
    "        figures.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### useful sample file/page combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# NF_SR2089 part 426010 on page 661\n",
    "\n",
    "##########\n",
    "#p = 570 #image only\n",
    "#p = 40 #fancy image\n",
    "#p = 1377 #contains comments\n",
    "#p = 1435 #index table at endb\n",
    "#p = 3 #table of contents\n",
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### useful documentation on parsing the 'char' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#def create_data(data, filename, path, page, header=0, subheader=1, body=5, write='false'):\n",
    "  # Here we go! Let's create the final table and put it in Excel\n",
    "\n",
    "p=857 #885\n",
    "row = 15\n",
    "\n",
    "#p=pg['pgstart']\n",
    "# Analyze the requested PDF file\n",
    "pa = pdf_analyze(filename=osp['f'], page=p-1, cm=prm['cm'], lm=prm['lm'], wm=prm['wm'], bf=prm['bf'], lo=prm['lo'])\n",
    "                                    #defaults: cm=2.0,   lm=0.5, wm=0.1, bf=0.5, lo=0.5\n",
    "print(pa['message'])\n",
    "\n",
    "# Extract data for the specified page\n",
    "d = extract_data(filename=osp['f'], page=p,layout=pa['layout'])\n",
    "print(d['message'])\n",
    "\n",
    "    \n",
    "data= d['data']\n",
    "\n",
    "#get the char piece from the dataset & write to the window\n",
    "char = data[row]['char'][0]\n",
    "#print(char)\n",
    "\n",
    "#split the first two based on space\n",
    "sp1 = char.split(' ',2)\n",
    "\n",
    "#look for the asterisk\n",
    "test_ast = (re.match('^[#*()&%$@!+-]\\s', sp1[2])) #or (re.match('^QR', sp1[2]))\n",
    "ast, body ='',''\n",
    "if test_ast:\n",
    "    ast, body = sp1[len(sp1)-1][:2], sp1[len(sp1)-1][2:]\n",
    "    sp1.pop(2)\n",
    "    sp1.extend([body])\n",
    "    print(sp1)\n",
    "\n",
    "    \n",
    "#test for matches\n",
    "test_desqt = (re.match('^[0-9]+', sp1[2])) or (re.match('^QR', sp1[2]))\n",
    "if test_desqt:\n",
    "    \n",
    "    #split the remaining desc/quantity field\n",
    "    qty, desc = sp1[len(sp1)-1][:2], sp1[len(sp1)-1][2:]\n",
    "\n",
    "    #get rid of the crappy desc/quantity combo field\n",
    "    sp1.pop(2)\n",
    "\n",
    "    #combine the new desc/qty fields back to the main output\n",
    "    sp1.extend([desc,qty, ast])\n",
    "    #sp1\n",
    "else:\n",
    "    sp1.extend([ast])\n",
    "\n",
    "sp1\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
